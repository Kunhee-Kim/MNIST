{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "layers = 2\n",
    "\"\"\"\n",
    "layer_features = np.ones(layers)\n",
    "layer_dropout = np.ones(layers)\n",
    "\"\"\"\n",
    "layer_features = np.array([128, 10])\n",
    "layer_dropout = np.array([0.25, 0.5]) # dropout = 0 if not used\n",
    "\n",
    "\n",
    "# mnist data\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_mnist = dsets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_mnist = dsets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_mnist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_mnist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "# device\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device=='cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, layers_size, output_size):\n",
    "        super(LinearNet, self).__init__()\n",
    "\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_size, layers_size[0])])\n",
    "        print('linears 0 :', self.linears[0])\n",
    "        self.linears.extend([nn.Linear(layers_size[i-1], layers_size[i]) for i in range(1, num_layers-1)])\n",
    "        self.linears.append(nn.Linear(layers_size[num_layers-2], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        for l in range(layers):\n",
    "\n",
    "                #self.dropout = nn.Dropout(layer_dropout[l])\n",
    "                #x = self.dropout(x)\n",
    "                #if l == 0 :\n",
    "                    #x = torch.flatten(x, 1)\n",
    "            self.fc1_ = self.linear_array.linears[0]\n",
    "            x = self.fc1_(x)\n",
    "            if l > 0:\n",
    "                x = (self.linear_array.linears[l])(x)\n",
    "            if l < layers-1 :\n",
    "                x = F.relu(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "        return output\n",
    "        \"\"\"\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "class CNN(nn.Module) :\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.dropout = None\n",
    "        self.linear_array = LinearNet(input_size=9216, num_layers=layers, layers_size=layer_features, output_size= 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(layer_dropout[0])\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "\n",
    "\n",
    "        for l in range(layers):\n",
    "\n",
    "            #self.dropout = nn.Dropout(layer_dropout[l])\n",
    "            #x = self.dropout(x)\n",
    "            #if l == 0 :\n",
    "                #x = torch.flatten(x, 1)\n",
    "            self.fc1_ = self.linear_array.linears[0]\n",
    "\n",
    "            if l == 0:\n",
    "                x = self.fc1_(x)\n",
    "            if l > 0:\n",
    "                x = (self.linear_array.linears[l])(x)\n",
    "            if l < layers-1 :\n",
    "                x = F.relu(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears 0 : Linear(in_features=9216, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.187231779\n",
      "[Epoch:    2] cost = 0.0505603552\n",
      "[Epoch:    3] cost = 0.0337810516\n",
      "[Epoch:    4] cost = 0.0240517054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-69-68031b7359dd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0mavg_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtraining_batch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/MNIST/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/MNIST/lib/python3.8/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m             \u001B[0mbeta1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbeta2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgroup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'betas'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m             F.adam(params_with_grad,\n\u001B[0m\u001B[1;32m    109\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/MNIST/lib/python3.8/site-packages/torch/optim/functional.py\u001B[0m in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0mdenom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexp_avg_sq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m         \u001B[0mstep_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mbias_correction1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m         \u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddcdiv_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexp_avg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdenom\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mstep_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "training_batch = len(train_dataloader)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss/training_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.053584\n",
      "accuracy : 0.988300\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "avg_loss_test = 0\n",
    "test_batch = len(test_dataloader)\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs_test, labels_test in test_dataloader:\n",
    "\n",
    "        inputs_test = inputs_test.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "        outputs_test = model(inputs_test)\n",
    "\n",
    "        loss_test = criterion(outputs_test, labels_test)\n",
    "        avg_loss_test+= loss_test/test_batch\n",
    "\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "        for label, prediction in zip(labels_test, predicted):\n",
    "            if label == prediction:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = 0\n",
    "if total != 0:\n",
    "    accuracy = correct / total\n",
    "\n",
    "print('test loss : %f'%avg_loss_test)\n",
    "print('accuracy : %f'%accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  [2 3] test :  [0 1] val : [4 5]\n",
      "train :  [0 1] test :  [2 3] val : [4 5]\n",
      "train :  [0 1] test :  [4 5] val : [2 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "kf = KFold(n_splits=3)\n",
    "for tr, te in kf.split(arr):\n",
    "    val = tr[2:]\n",
    "    tr = tr[:2]\n",
    "    print('train : ', tr, 'test : ', te, 'val :', val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_mnist))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  11  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  26  27  29  32  33  34  35  36  37  38  39  41  42\n",
      "  43  46  47  48  49  50  51  52  53  55  56  59  60  61  62  64  65  68\n",
      "  69  70  71  72  73  75  76  77  78  79  80  81  82  83  86  87  90  94\n",
      "  96  97  98 100 101 102 105 109 110 112 114 117 118 120 122 123 124 126\n",
      " 127 128 130 131 133 135 136 137 138 139]\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "splits = 4\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits = splits, shuffle = True, random_state= True)\n",
    "\n",
    "train_index1 = None\n",
    "val_index1 = None\n",
    "\n",
    "for tr, val in kfold.split(train_mnist):\n",
    "    train_index1 = tr\n",
    "    val_index1 = val\n",
    "    break\n",
    "\n",
    "\n",
    "#train_dataloader_1 = DataLoader(train_mnist(train1), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "#val_dataloader_1 = DataLoader(train_mnist[val1], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(train_index1[:100])\n",
    "train1_mnist = torch.utils.data.Subset(train_mnist, train_index1)\n",
    "val1_mnist = torch.utils.data.Subset(train_mnist, val_index1)\n",
    "\n",
    "print(len(train1_mnist))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "train_dataloader_1 = DataLoader(train1_mnist, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader_1 = DataLoader(val1_mnist, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "linears 0 : Linear(in_features=9216, out_features=128, bias=True)\n",
      "[Epoch:    1] cost = 0.204892352\n",
      "[Epoch:    2] cost = 0.0551045835\n",
      "[Epoch:    3] cost = 0.0344592296\n",
      "[Epoch:    4] cost = 0.0254920963\n",
      "[Epoch:    5] cost = 0.0174343381\n",
      "[Epoch:    6] cost = 0.0143420631\n",
      "[Epoch:    7] cost = 0.0131712202\n",
      "[Epoch:    8] cost = 0.0103758061\n",
      "[Epoch:    9] cost = 0.00980160385\n",
      "[Epoch:   10] cost = 0.00815232564\n",
      "[Epoch:   11] cost = 0.00846363977\n",
      "[Epoch:   12] cost = 0.00520470878\n",
      "[Epoch:   13] cost = 0.00490274932\n",
      "[Epoch:   14] cost = 0.00857084431\n",
      "[Epoch:   15] cost = 0.00497053238\n"
     ]
    }
   ],
   "source": [
    "training_batch = len(train_dataloader_1)\n",
    "print(training_batch)\n",
    "model_tr = CNN()\n",
    "criterion_tr = nn.CrossEntropyLoss()\n",
    "optimizer_tr = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader_1:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer_tr.zero_grad()\n",
    "\n",
    "        outputs = model_tr(inputs)\n",
    "        loss = criterion_tr(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_tr.step()\n",
    "\n",
    "        avg_loss += loss/training_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.066538\n",
      "accuracy : 0.986533\n"
     ]
    }
   ],
   "source": [
    "avg_loss_val1 = 0\n",
    "val_batch = len(val_dataloader_1)\n",
    "total1 = 0\n",
    "correct1 = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs_test, labels_test in val_dataloader_1:\n",
    "\n",
    "        inputs_test = inputs_test.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "        outputs_test = model_tr(inputs_test)\n",
    "\n",
    "        loss_test = criterion_tr(outputs_test, labels_test)\n",
    "        avg_loss_val1+= loss_test/val_batch\n",
    "\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "        for label, prediction in zip(labels_test, predicted):\n",
    "            if label == prediction:\n",
    "                correct1 += 1\n",
    "            total1 += 1\n",
    "\n",
    "accuracy1 = 0\n",
    "if total1 != 0:\n",
    "    accuracy1 = correct1 / total1\n",
    "\n",
    "print('test loss : %f'%avg_loss_val1)\n",
    "print('accuracy : %f'%accuracy1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears 0 : Linear(in_features=9216, out_features=128, bias=True)\n",
      "[Epoch:    1] cost = 0.221597999\n",
      "[Epoch:    2] cost = 0.0555653647\n"
     ]
    }
   ],
   "source": [
    "def training(tr_dataloader, cnn_model):\n",
    "\n",
    "    training_batch = len(tr_dataloader)\n",
    "    tr_criterion = nn.CrossEntropyLoss()\n",
    "    tr_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    for epoch in range(2):\n",
    "        avg_loss = 0\n",
    "\n",
    "        for inputs, labels in tr_dataloader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            tr_optimizer.zero_grad()\n",
    "\n",
    "            outputs = cnn_model(inputs)\n",
    "            loss = tr_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            tr_optimizer.step()\n",
    "\n",
    "            avg_loss += loss/training_batch\n",
    "\n",
    "        print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_loss))\n",
    "\n",
    "model_tr2 = CNN()\n",
    "training(train_dataloader_1, model_tr2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.059631\n",
      "accuracy : 0.982467\n"
     ]
    }
   ],
   "source": [
    "def testing(te_dataloader, cnn_model):\n",
    "    te_avg_loss = 0\n",
    "    te_batch = len(val_dataloader_1)\n",
    "    te_total = 0\n",
    "    te_correct = 0\n",
    "\n",
    "    tr_criterion = nn.CrossEntropyLoss()\n",
    "    tr_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs_te, labels_te in te_dataloader:\n",
    "\n",
    "            inputs_te = inputs_te.to(device)\n",
    "            labels_te = labels_te.to(device)\n",
    "            outputs_te = cnn_model(inputs_te)\n",
    "\n",
    "            loss_te = tr_criterion(outputs_te, labels_te)\n",
    "            te_avg_loss+= loss_te/te_batch\n",
    "\n",
    "            _, predicted = torch.max(outputs_te.data, 1)\n",
    "            for label, prediction in zip(labels_te, predicted):\n",
    "                if label == prediction:\n",
    "                    te_correct += 1\n",
    "                te_total += 1\n",
    "\n",
    "    te_accuracy = 0\n",
    "    if te_total != 0:\n",
    "        te_accuracy = te_correct / te_total\n",
    "\n",
    "    print('test loss : %f'%te_avg_loss)\n",
    "    print('accuracy : %f'%te_accuracy)\n",
    "\n",
    "testing(val_dataloader_1, model_tr2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}